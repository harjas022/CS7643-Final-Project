{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "domestic-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from IPython.core.display import display, HTML\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import ast\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import *\n",
    "from model_training_utils import train as training_loop\n",
    "from model_training_utils import validate as validation_loop\n",
    "from model_training_utils import hp_grid_search\n",
    "from models import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "employed-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "t= torch.tensor(cv2.imread('./images/resized/1.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coated-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_movedim= t.movedim(2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "previous-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_reshape= t.reshape((3,500,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "strategic-elevation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training observations:  32\n",
      "Number of validation observations:  11\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv('./datasets/annotations_map.csv', converters={'new_bb': from_np_array})\n",
    "df_train = df.reset_index()\n",
    "X = df_train[['new_path','new_bb']]\n",
    "Y = df_train['class']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "print('Number of training observations: ', X_train.shape[0])\n",
    "print('Number of validation observations: ', X_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "laughing-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Hyperparameters -- Currently setting values that we can modify\n",
    "loss_type = \"l1\"\n",
    "learning_rate = 0.001\n",
    "momentum = 0.5\n",
    "reg = 1e-2\n",
    "training_batch_size= 12\n",
    "validation_batch_size= 5\n",
    "model= \"SimpleCNN\"\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "model= CNN()\n",
    "train_ds = WaldoDataset(X_train['new_path'],X_train['new_bb'] ,y_train)\n",
    "valid_ds = WaldoDataset(X_val['new_path'],X_val['new_bb'],y_val)\n",
    "train_dl = DataLoader(train_ds, batch_size=training_batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=validation_batch_size)\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate,momentum=momentum,weight_decay=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hybrid-graduation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew.lofgreen/Documents/OMSCS/cs7643/dl7643/CS7643-Final-Project/model_training_utils.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_bb= torch.tensor(y_bb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------------\n",
      "Training Loss for Epoch 0: 83.59792900085449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew.lofgreen/Documents/OMSCS/cs7643/dl7643/CS7643-Final-Project/model_training_utils.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_bb= torch.tensor(y_bb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss for Epoch 0: 614.0933227539062\n",
      " \n",
      "--------------------------------------------------------\n",
      "Training Loss for Epoch 1: 81.57487869262695\n",
      "Validation Loss for Epoch 1: 609.1585235595703\n"
     ]
    }
   ],
   "source": [
    "training_loop(model, optimizer, train_dl, valid_dl, criterion, 2, False, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "electoral-portugal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>new_path</th>\n",
       "      <th>new_bb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./images/base/1.jpg</td>\n",
       "      <td>2048</td>\n",
       "      <td>1251</td>\n",
       "      <td>waldo</td>\n",
       "      <td>706</td>\n",
       "      <td>513</td>\n",
       "      <td>743</td>\n",
       "      <td>562</td>\n",
       "      <td>images/resized/1.jpg</td>\n",
       "      <td>[172 205 181 225]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./images/base/10.jpg</td>\n",
       "      <td>1600</td>\n",
       "      <td>980</td>\n",
       "      <td>waldo</td>\n",
       "      <td>715</td>\n",
       "      <td>157</td>\n",
       "      <td>733</td>\n",
       "      <td>181</td>\n",
       "      <td>images/resized/10.jpg</td>\n",
       "      <td>[223  80 229  92]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./images/base/11.jpg</td>\n",
       "      <td>2828</td>\n",
       "      <td>1828</td>\n",
       "      <td>waldo</td>\n",
       "      <td>460</td>\n",
       "      <td>1530</td>\n",
       "      <td>482</td>\n",
       "      <td>1557</td>\n",
       "      <td>images/resized/11.jpg</td>\n",
       "      <td>[ 81 418  85 426]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./images/base/12.jpg</td>\n",
       "      <td>1276</td>\n",
       "      <td>1754</td>\n",
       "      <td>waldo</td>\n",
       "      <td>846</td>\n",
       "      <td>517</td>\n",
       "      <td>878</td>\n",
       "      <td>563</td>\n",
       "      <td>images/resized/12.jpg</td>\n",
       "      <td>[332 147 344 160]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./images/base/13.jpg</td>\n",
       "      <td>1280</td>\n",
       "      <td>864</td>\n",
       "      <td>waldo</td>\n",
       "      <td>932</td>\n",
       "      <td>274</td>\n",
       "      <td>942</td>\n",
       "      <td>288</td>\n",
       "      <td>images/resized/13.jpg</td>\n",
       "      <td>[364 159 368 167]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./images/base/14.jpg</td>\n",
       "      <td>1700</td>\n",
       "      <td>2340</td>\n",
       "      <td>waldo</td>\n",
       "      <td>197</td>\n",
       "      <td>1878</td>\n",
       "      <td>237</td>\n",
       "      <td>1919</td>\n",
       "      <td>images/resized/14.jpg</td>\n",
       "      <td>[ 58 401  70 410]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>./images/base/15.jpg</td>\n",
       "      <td>1600</td>\n",
       "      <td>1006</td>\n",
       "      <td>waldo</td>\n",
       "      <td>297</td>\n",
       "      <td>428</td>\n",
       "      <td>311</td>\n",
       "      <td>445</td>\n",
       "      <td>images/resized/15.jpg</td>\n",
       "      <td>[ 93 213  97 221]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>./images/base/17.jpg</td>\n",
       "      <td>1599</td>\n",
       "      <td>1230</td>\n",
       "      <td>waldo</td>\n",
       "      <td>751</td>\n",
       "      <td>250</td>\n",
       "      <td>772</td>\n",
       "      <td>281</td>\n",
       "      <td>images/resized/17.jpg</td>\n",
       "      <td>[235 102 241 114]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>./images/base/18.jpg</td>\n",
       "      <td>1590</td>\n",
       "      <td>981</td>\n",
       "      <td>waldo</td>\n",
       "      <td>1259</td>\n",
       "      <td>75</td>\n",
       "      <td>1290</td>\n",
       "      <td>111</td>\n",
       "      <td>images/resized/18.jpg</td>\n",
       "      <td>[396  38 406  57]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>./images/base/19.jpg</td>\n",
       "      <td>1280</td>\n",
       "      <td>864</td>\n",
       "      <td>waldo</td>\n",
       "      <td>338</td>\n",
       "      <td>623</td>\n",
       "      <td>351</td>\n",
       "      <td>640</td>\n",
       "      <td>images/resized/19.jpg</td>\n",
       "      <td>[132 361 137 370]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename  width  height  class  xmin  ymin  xmax  ymax  \\\n",
       "0   ./images/base/1.jpg   2048    1251  waldo   706   513   743   562   \n",
       "1  ./images/base/10.jpg   1600     980  waldo   715   157   733   181   \n",
       "2  ./images/base/11.jpg   2828    1828  waldo   460  1530   482  1557   \n",
       "3  ./images/base/12.jpg   1276    1754  waldo   846   517   878   563   \n",
       "4  ./images/base/13.jpg   1280     864  waldo   932   274   942   288   \n",
       "5  ./images/base/14.jpg   1700    2340  waldo   197  1878   237  1919   \n",
       "6  ./images/base/15.jpg   1600    1006  waldo   297   428   311   445   \n",
       "7  ./images/base/17.jpg   1599    1230  waldo   751   250   772   281   \n",
       "8  ./images/base/18.jpg   1590     981  waldo  1259    75  1290   111   \n",
       "9  ./images/base/19.jpg   1280     864  waldo   338   623   351   640   \n",
       "\n",
       "                new_path             new_bb  \n",
       "0   images/resized/1.jpg  [172 205 181 225]  \n",
       "1  images/resized/10.jpg  [223  80 229  92]  \n",
       "2  images/resized/11.jpg  [ 81 418  85 426]  \n",
       "3  images/resized/12.jpg  [332 147 344 160]  \n",
       "4  images/resized/13.jpg  [364 159 368 167]  \n",
       "5  images/resized/14.jpg  [ 58 401  70 410]  \n",
       "6  images/resized/15.jpg  [ 93 213  97 221]  \n",
       "7  images/resized/17.jpg  [235 102 241 114]  \n",
       "8  images/resized/18.jpg  [396  38 406  57]  \n",
       "9  images/resized/19.jpg  [132 361 137 370]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img= cv2.imread(\"./images/resized/1.jpg\")\n",
    "img= torch.tensor(img)\n",
    "img= torch.unsqueeze(img, dim= 0)\n",
    "img= img.float()\n",
    "df= pd.read_csv('./datasets/annotations_map.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "turkish-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "output= model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "helpful-black",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13458.9287,  9587.8594, 15120.9600, 10811.1416]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "supreme-buffer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15391.2998, grad_fn=<L1LossBackward>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(output, torch.tensor([[172,205,181,225]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-surrey",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
