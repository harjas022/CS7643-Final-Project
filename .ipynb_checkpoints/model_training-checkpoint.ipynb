{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adverse-injection",
   "metadata": {},
   "source": [
    "# Model Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from IPython.core.display import display, HTML\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import ast\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# repeatable functions housed in the utils file and imported here\n",
    "from utils import *\n",
    "from model_training_utils import train as training_loop\n",
    "from model_training_utils import validate as validation_loop\n",
    "from model_training_utils import hp_grid_search\n",
    "from models import CNN, YOLO, VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-surveillance",
   "metadata": {},
   "source": [
    "## Create Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('./datasets/annotations_map.csv', converters={'new_bb': from_np_array})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-sunrise",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = df.reset_index()\n",
    "X = df_train[['new_path','new_bb']]\n",
    "Y = df_train['class']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "print('Number of training observations: ', X_train.shape[0])\n",
    "print('Number of validation observations: ', X_val.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-pointer",
   "metadata": {},
   "source": [
    "## Build RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-tradition",
   "metadata": {},
   "source": [
    "## Build VGG\n",
    "Implementation of VGG-16 architecture based on https://neurohive.io/en/popular-networks/vgg16/. VGG-16 is known for its high accuracy and speen on object detection tasks, largely attributed to its 3x3 kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-reality",
   "metadata": {},
   "source": [
    "## Build YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-marks",
   "metadata": {},
   "source": [
    "#### The Design of the YOLO NN was taken from the following paper:\n",
    "\n",
    "https://arxiv.org/pdf/1506.02640.pdf - \"You Only Look Once: Unified, Real-Time Object Detection\" by Redmon, Divvala, Girshick, and Farhadi\n",
    "\n",
    "The following article is YOLO V2:\n",
    "https://arxiv.org/pdf/1612.08242v1.pdf - \"YOLO 9000: Better, Faster, Stronger\" by Redmon, and Farhadi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(YOLO, self).__init__()\n",
    "        \n",
    "        # First\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1, padding=1)\n",
    "        self.pooling = nn.AvgPool2d(2, 2)\n",
    "        \n",
    "        # Second\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, padding=1)\n",
    "        \n",
    "        # Third\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 64, 1, 1)\n",
    "        \n",
    "        # Fourth\n",
    "        self.conv5 = nn.Conv2d(128, 256, 3, 1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 128, 1, 1)\n",
    "        \n",
    "        # Fifth\n",
    "        self.conv7 = nn.Conv2d(256, 512, 3, 1, padding=1)\n",
    "        self.conv8 = nn.Conv2d(512, 256, 1, 1)\n",
    "        \n",
    "        # Sixth\n",
    "        self.conv9 = nn.Conv2d(512, 1024, 3, 1, padding=1)\n",
    "        self.conv10 = nn.Conv2d(1024, 512, 1, 1)\n",
    "        \n",
    "        # Final\n",
    "        self.conv11 = nn.Conv2d(1024, 1000, 1, 1)\n",
    "        \n",
    "        # FC Layer and Softmax\n",
    "        self.FC = nn.Linear(1000, 4)\n",
    "        \n",
    "        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size, h, w, channel = x.shape\n",
    "        x= x.reshape(batch_size, channel, h, w)\n",
    "        \n",
    "        output = self.conv1(x) # 500\n",
    "        output = self.pooling(output) # 250\n",
    "        \n",
    "        output = self.conv2(output) # 250\n",
    "        output = self.pooling(output) # 125\n",
    "        \n",
    "        output = self.conv3(output) # 125\n",
    "        output = self.conv4(output) # 125\n",
    "        output = self.conv3(output) # 125\n",
    "        output = self.pooling(output) # 62\n",
    "        \n",
    "        output = self.conv5(output) # 62\n",
    "        output = self.conv6(output) # 62\n",
    "        output = self.conv5(output) # 62\n",
    "        output = self.pooling(output) # 31\n",
    "        \n",
    "        output = self.conv7(output) # 31\n",
    "        output = self.conv8(output) # 31\n",
    "        output = self.conv7(output) # 31\n",
    "        output = self.conv8(output) # 31\n",
    "        output = self.conv7(output) # 31\n",
    "        output = self.pooling(output) # 15\n",
    "        \n",
    "        output = self.conv9(output) # 15\n",
    "        output = self.conv10(output) # 15\n",
    "        output = self.conv9(output) # 15\n",
    "        output = self.conv10(output) # 15\n",
    "        output = self.conv9(output) # 15\n",
    "        \n",
    "        output = self.conv11(output)\n",
    "        output = output.mean([2, 3])\n",
    "        \n",
    "        output = self.FC(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-snowboard",
   "metadata": {},
   "source": [
    "# Reference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SingleLinear(nn.Module):\n",
    "    ## Initialization of the model\n",
    "    def __init__(self):\n",
    "        super(SingleLinear, self).__init__()\n",
    "        self.linear= nn.Linear(1000, 4)\n",
    "    ## Defining the forward function\n",
    "    def forward(self, x):\n",
    "        output= self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(torchvision.models.inception_v3(pretrained = True, aux_logits=False), SingleLinear())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-export",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n",
    "\n",
    "- Structured similarly to main.py file from pytorch part of A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Hyperparameters -- Currently setting values that we can modify\n",
    "loss_type = \"l1\"\n",
    "learning_rate = 0.001\n",
    "momentum = 0.5\n",
    "reg = 1e-2\n",
    "\n",
    "training_batch_size= 5\n",
    "validation_batch_size= 5\n",
    "\n",
    "model= \"VGG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss_type == \"l1\":\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "if loss_type == \"l2\":\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "if model== \"SimpleCNN\":\n",
    "    model= CNN()\n",
    "    \n",
    "elif model == \"YOLO\":\n",
    "    model = YOLO()\n",
    "elif model == \"VGG\":\n",
    "    model = vgg\n",
    "    \n",
    "elif model == \"VGG\":\n",
    "    model = vgg\n",
    "    \n",
    "train_ds = WaldoDataset(X_train['new_path'],X_train['new_bb'] ,y_train)\n",
    "valid_ds = WaldoDataset(X_val['new_path'],X_val['new_bb'],y_val)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=training_batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=validation_batch_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate,\n",
    "                            momentum=momentum,\n",
    "                            weight_decay=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-hollow",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_loop(model= model, optimizer = optimizer, train_dl= train_dl, valid_dl=valid_dl, epochs= 20, criterion= criterion, verbose= True, return_loss= False, pretrained_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type= [\"l1\"]\n",
    "learning_rate= [0.001,0.0001]\n",
    "momentum = [0.9]\n",
    "reg = [0.01]\n",
    "batch_size= [10]\n",
    "\n",
    "all_training_loss, all_validation_loss= hp_grid_search(model_type= \"SimpleCNN\", \n",
    "               lr_list=learning_rate, \n",
    "               momentum_list=momentum, \n",
    "               reg_list=reg, \n",
    "               batch_size_list=batch_size,\n",
    "               train_ds= train_ds,\n",
    "               valid_ds= valid_ds,\n",
    "               optimizer= optimizer, \n",
    "               loss_type_list=loss_type,\n",
    "               epochs= 10,\n",
    "               save_all_plots=\"Yes\", \n",
    "               save_final_plot=\"Yes\",\n",
    "               final_plot_prefix=\"Test\", \n",
    "               return_all_loss= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-cooperation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
