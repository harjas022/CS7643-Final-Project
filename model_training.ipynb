{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exempt-equity",
   "metadata": {},
   "source": [
    "# Model Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from IPython.core.display import display, HTML\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import ast\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# repeatable functions housed in the utils file and imported here\n",
    "from utils import *\n",
    "from model_training_utils import train as training_loop\n",
    "from model_training_utils import validate as validation_loop\n",
    "from model_training_utils import hp_grid_search\n",
    "from models import CNN, YOLO, VGG, PreTrainedVGG, SimpleVGG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-namibia",
   "metadata": {},
   "source": [
    "## Create Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('./datasets/annotations_map.csv', converters={'new_bb': from_np_array})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-sunrise",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = df.reset_index()\n",
    "X = df_train[['new_path','new_bb']]\n",
    "Y = df_train['class']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "print('Number of training observations: ', X_train.shape[0])\n",
    "print('Number of validation observations: ', X_val.shape[0])"
   ]
  },
  {
   "source": [
    "## Build Simple CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-tradition",
   "metadata": {},
   "source": [
    "## Build VGG\n",
    "Implementation of VGG-16 architecture based on https://neurohive.io/en/popular-networks/vgg16/. VGG-16 is known for its high accuracy and speen on object detection tasks, largely attributed to its 3x3 kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG()"
   ]
  },
  {
   "source": [
    "## Pre-trained VGG\n",
    "A pytorch VGG-16 model that was pre-trained on object identification. Utilized for benchmarking our own implementations against."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preTrainedVGG = PreTrainedVGG()"
   ]
  },
  {
   "source": [
    "## Build Simple VGG\n",
    "Our simplified VGG based model that attempts to achieve better performance by using less layers. The idea behind this was that the many layers and transformations in VGG can cause it to struggle with smaller objects."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_vgg = SimpleVGG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-domestic",
   "metadata": {},
   "source": [
    "## Build YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-catch",
   "metadata": {},
   "source": [
    "#### The Design of the YOLO NN was taken from the following paper:\n",
    "\n",
    "https://arxiv.org/pdf/1506.02640.pdf - \"You Only Look Once: Unified, Real-Time Object Detection\" by Redmon, Divvala, Girshick, and Farhadi\n",
    "\n",
    "The following article is YOLO V2:\n",
    "https://arxiv.org/pdf/1612.08242v1.pdf - \"YOLO 9000: Better, Faster, Stronger\" by Redmon, and Farhadi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = YOLO()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-snowboard",
   "metadata": {},
   "source": [
    "# Reference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SingleLinear(nn.Module):\n",
    "    ## Initialization of the model\n",
    "    def __init__(self):\n",
    "        super(SingleLinear, self).__init__()\n",
    "        self.linear= nn.Linear(1000, 4)\n",
    "    ## Defining the forward function\n",
    "    def forward(self, x):\n",
    "        output= self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(torchvision.models.inception_v3(pretrained = True, aux_logits=False), SingleLinear())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-export",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n",
    "\n",
    "- Structured similarly to main.py file from pytorch part of A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Hyperparameters -- Currently setting values that we can modify\n",
    "loss_type = \"l1\"\n",
    "learning_rate = 0.001\n",
    "momentum = 0.5\n",
    "reg = 1e-2\n",
    "\n",
    "training_batch_size = 40\n",
    "validation_batch_size = 40\n",
    "\n",
    "model_type= \"SimpleCNN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss_type == \"l1\":\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "if loss_type == \"l2\":\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "if model_type == \"SimpleCNN\":\n",
    "    model = simple_cnn\n",
    "elif model_type == \"YOLO\":\n",
    "    model = yolo\n",
    "elif model_type == \"VGG\":\n",
    "    model = vgg\n",
    "elif model_type == \"PreTrainedVGG\":\n",
    "    model = preTrainedVGG\n",
    "elif model_type == \"SimpleVGG\":\n",
    "    model = simple_vgg\n",
    "    \n",
    "train_ds = WaldoDataset(X_train['new_path'],X_train['new_bb'] ,y_train)\n",
    "valid_ds = WaldoDataset(X_val['new_path'],X_val['new_bb'],y_val)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=training_batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=validation_batch_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate,\n",
    "                            momentum=momentum,\n",
    "                            weight_decay=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-hollow",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_loop(model= model, optimizer = optimizer, train_dl= train_dl, valid_dl=valid_dl, epochs= 20, criterion= criterion, verbose= True, return_loss= False, pretrained_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type= [\"l1\"]\n",
    "learning_rate= [0.001,0.0001]\n",
    "momentum = [0.9]\n",
    "reg = [0.01]\n",
    "batch_size= [10]\n",
    "\n",
    "all_training_loss, all_validation_loss= hp_grid_search(model_type= \"SimpleCNN\", \n",
    "               lr_list=learning_rate, \n",
    "               momentum_list=momentum, \n",
    "               reg_list=reg, \n",
    "               batch_size_list=batch_size,\n",
    "               train_ds= train_ds,\n",
    "               valid_ds= valid_ds,\n",
    "               optimizer= optimizer, \n",
    "               loss_type_list=loss_type,\n",
    "               epochs= 10,\n",
    "               save_all_plots=\"Yes\", \n",
    "               save_final_plot=\"Yes\",\n",
    "               final_plot_prefix=\"Test\", \n",
    "               return_all_loss= True)"
   ]
  },
  {
   "source": [
    "## Generate GradCAM\n",
    "Generates GradCAM images for Simple CNN and saves them to images/SimpleCNNResizedGradCam"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ref: https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82\n",
    "if model_type == \"SimpleCNN\":\n",
    "    for x, y_bb in train_dl:\n",
    "        x = x.float()\n",
    "        for i in range(len(x)):\n",
    "            input = x[i]\n",
    "            input = torch.unsqueeze(input, dim=0)\n",
    "            output = model(input)\n",
    "\n",
    "            output.sum().backward()\n",
    "\n",
    "            gradients = model.get_activations_gradient()\n",
    "\n",
    "            # not sure about this dims\n",
    "            pooled_gradients = torch.mean(gradients, dim=1)\n",
    "\n",
    "            x_perm = x.permute(1,0,2,3)\n",
    "            activations = model.get_activations(x_perm).detach()\n",
    "\n",
    "\n",
    "            for k in range(len(pooled_gradients)):\n",
    "                activations[:, k] *= pooled_gradients[k]\n",
    "\n",
    "            heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "            heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "            heatmap /= torch.max(heatmap)\n",
    "\n",
    "            img = cv2.imread('./images/resized/' + str(i + 1) + '.jpg')\n",
    "            heatmap = cv2.resize(np.float32(heatmap), (img.shape[1], img.shape[0]))\n",
    "            heatmap = np.uint8(255 * heatmap)\n",
    "            heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "            superimposed_img = heatmap * .8 + img\n",
    "            cv2.imwrite('./images/' + model_type + 'ResizedGradCam/' + str(i + 1) + '.jpg', superimposed_img)\n",
    "else:\n",
    "    print('GradCAM requires significant setup in the model and currently only works for SimpleCNN')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
