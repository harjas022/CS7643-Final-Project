{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "settled-silicon",
   "metadata": {},
   "source": [
    "# Model Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "backed-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from IPython.core.display import display, HTML\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import ast\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# repeatable functions housed in the utils file and imported here\n",
    "from utils import *\n",
    "from model_training_utils import train as training_loop\n",
    "from model_training_utils import validate as validation_loop\n",
    "from model_training_utils import hp_grid_search\n",
    "from models import CNN, YOLO, VGG, PreTrainedVGG, SimpleVGG, SingleLinear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-apache",
   "metadata": {},
   "source": [
    "## Create Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "banner-glass",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('./datasets/annotations_map.csv', converters={'new_bb': from_np_array})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "becoming-tract",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training observations:  32\n",
      "Number of validation observations:  11\n"
     ]
    }
   ],
   "source": [
    "df_train = df.reset_index()\n",
    "X = df_train[['new_path','new_bb']]\n",
    "Y = df_train['class']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "print('Number of training observations: ', X_train.shape[0])\n",
    "print('Number of validation observations: ', X_val.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-livestock",
   "metadata": {},
   "source": [
    "## Build Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "detected-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-basis",
   "metadata": {},
   "source": [
    "## Build VGG\n",
    "Implementation of VGG-16 architecture based on https://neurohive.io/en/popular-networks/vgg16/. VGG-16 is known for its high accuracy and speen on object detection tasks, largely attributed to its 3x3 kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "enabling-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-gospel",
   "metadata": {},
   "source": [
    "## Pre-trained VGG\n",
    "A pytorch VGG-16 model that was pre-trained on object identification. Utilized for benchmarking our own implementations against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sacred-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "preTrainedVGG = PreTrainedVGG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-pressure",
   "metadata": {},
   "source": [
    "## Build Simple VGG\n",
    "Our simplified VGG based model that attempts to achieve better performance by using less layers. The idea behind this was that the many layers and transformations in VGG can cause it to struggle with smaller objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "electoral-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_vgg = SimpleVGG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-credit",
   "metadata": {},
   "source": [
    "## Build YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-laser",
   "metadata": {},
   "source": [
    "#### The Design of the YOLO NN was taken from the following paper:\n",
    "\n",
    "https://arxiv.org/pdf/1506.02640.pdf - \"You Only Look Once: Unified, Real-Time Object Detection\" by Redmon, Divvala, Girshick, and Farhadi\n",
    "\n",
    "The following article is YOLO V2:\n",
    "https://arxiv.org/pdf/1612.08242v1.pdf - \"YOLO 9000: Better, Faster, Stronger\" by Redmon, and Farhadi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "premium-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = YOLO()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-chinese",
   "metadata": {},
   "source": [
    "# Reference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "billion-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "miniature-stocks",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /Users/andrew.lofgreen/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f723e28382da424e9eabf56fefbe26bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/104M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inception_v3 = nn.Sequential(torchvision.models.inception_v3(pretrained = True, aux_logits=False), SingleLinear())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-moral",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n",
    "\n",
    "- Structured similarly to main.py file from pytorch part of A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fancy-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Hyperparameters -- Currently setting values that we can modify\n",
    "loss_type = \"l1\"\n",
    "learning_rate = 0.001\n",
    "momentum = 0.5\n",
    "reg = 1e-2\n",
    "\n",
    "training_batch_size = 40\n",
    "validation_batch_size = 40\n",
    "\n",
    "model_type = \"Inception\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "impossible-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss_type == \"l1\":\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "if loss_type == \"l2\":\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "if model_type == \"SimpleCNN\":\n",
    "    model = simple_cnn\n",
    "elif model_type == \"YOLO\":\n",
    "    model = yolo\n",
    "elif model_type == \"VGG\":\n",
    "    model = vgg\n",
    "elif model_type == \"PreTrainedVGG\":\n",
    "    model = preTrainedVGG\n",
    "elif model_type == \"SimpleVGG\":\n",
    "    model = simple_vgg\n",
    "elif model_type == \"Inception\":\n",
    "    model = inception_v3\n",
    "    \n",
    "train_ds = WaldoDataset(X_train['new_path'],X_train['new_bb'] ,y_train)\n",
    "valid_ds = WaldoDataset(X_val['new_path'],X_val['new_bb'],y_val)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=training_batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=validation_batch_size)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate,\n",
    "                            momentum=momentum,\n",
    "                            weight_decay=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-spine",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew.lofgreen/Documents/OMSCS/cs7643/dl7643/CS7643-Final-Project/model_training_utils.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_bb= torch.tensor(y_bb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------------\n",
      "Training Loss for Epoch 0: 6.833187103271484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew.lofgreen/Documents/OMSCS/cs7643/dl7643/CS7643-Final-Project/model_training_utils.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_bb= torch.tensor(y_bb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss for Epoch 0: 20.37358925559304\n",
      " \n",
      "--------------------------------------------------------\n",
      "Training Loss for Epoch 1: 6.831632614135742\n",
      "Validation Loss for Epoch 1: 20.368273648348723\n",
      " \n",
      "--------------------------------------------------------\n",
      "Training Loss for Epoch 2: 6.831934928894043\n",
      "Validation Loss for Epoch 2: 20.36471141468395\n",
      " \n",
      "--------------------------------------------------------\n",
      "Training Loss for Epoch 3: 6.830149173736572\n",
      "Validation Loss for Epoch 3: 20.3606220592152\n",
      " \n",
      "--------------------------------------------------------\n",
      "Training Loss for Epoch 4: 6.829981803894043\n",
      "Validation Loss for Epoch 4: 20.35437150435014\n",
      " \n",
      "--------------------------------------------------------\n",
      "Training Loss for Epoch 5: 6.827932357788086\n",
      "Validation Loss for Epoch 5: 20.349197387695312\n",
      " \n",
      "--------------------------------------------------------\n",
      "Training Loss for Epoch 6: 6.827206611633301\n",
      "Validation Loss for Epoch 6: 20.344254927201703\n",
      " \n",
      "--------------------------------------------------------\n",
      "Training Loss for Epoch 7: 6.825997352600098\n",
      "Validation Loss for Epoch 7: 20.33981600674716\n",
      " \n",
      "--------------------------------------------------------\n",
      "Training Loss for Epoch 8: 6.824359893798828\n",
      "Validation Loss for Epoch 8: 20.335173173384234\n"
     ]
    }
   ],
   "source": [
    "training_loop(model_type=model_type, model= model, optimizer = optimizer, train_dl= train_dl, valid_dl=valid_dl, epochs= 20, criterion= criterion, verbose= True, return_loss= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type= [\"l1\"]\n",
    "learning_rate= [0.001,0.0001]\n",
    "momentum = [0.9]\n",
    "reg = [0.01]\n",
    "batch_size= [10]\n",
    "\n",
    "all_training_loss, all_validation_loss= hp_grid_search(model_type= \"SimpleCNN\", \n",
    "               lr_list=learning_rate, \n",
    "               momentum_list=momentum, \n",
    "               reg_list=reg, \n",
    "               batch_size_list=batch_size,\n",
    "               train_ds= train_ds,\n",
    "               valid_ds= valid_ds,\n",
    "               optimizer= optimizer, \n",
    "               loss_type_list=loss_type,\n",
    "               epochs= 10,\n",
    "               save_all_plots=\"Yes\", \n",
    "               save_final_plot=\"Yes\",\n",
    "               final_plot_prefix=\"Test\", \n",
    "               return_all_loss= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-fault",
   "metadata": {},
   "source": [
    "## Generate GradCAM\n",
    "Generates GradCAM images for Simple CNN and saves them to images/SimpleCNNResizedGradCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam(model_type=model_type, model=model, train_dl=train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-animal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
